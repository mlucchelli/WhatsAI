{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setp 1: Clean chats avoiding white lines\n",
    "#\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Clean chats avoiding white lines\n",
    "\n",
    "# File paths\n",
    "input_file_path = os.getenv(\"ORIGINAL_CHAT_FILE\")\n",
    "cleaned_file_path = os.getenv(\"CLEANED_CHAT_FILE\")\n",
    "\n",
    "# Function to process and concatenate messages\n",
    "def process_chat(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        previous_line = None\n",
    "\n",
    "        for line in infile:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            # Check if the line starts with \"[\", indicating a timestamp\n",
    "            if stripped_line.startswith(\"[\"):\n",
    "                # Write the previous accumulated message to the output file\n",
    "                if previous_line:\n",
    "                    outfile.write(previous_line + \"\\n\")\n",
    "                previous_line = stripped_line\n",
    "            else:\n",
    "                # Concatenate non-timestamp lines with the previous line\n",
    "                if previous_line:\n",
    "                    previous_line += \". \" + stripped_line\n",
    "                else:\n",
    "                    previous_line = stripped_line\n",
    "\n",
    "        # Write the last accumulated message to the output file\n",
    "        if previous_line:\n",
    "            outfile.write(previous_line + \"\\n\")\n",
    "\n",
    "process_chat(input_file_path, cleaned_file_path)\n",
    "print(f\"Processed chat saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create one-line conversations for CSV training\n",
    "#\n",
    "\n",
    "#import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "# File paths\n",
    "cleaned_file_path = os.getenv(\"CLEANED_CHAT_FILE\")\n",
    "conversation_file_path = os.getenv(\"CONVERSATION_CHAT_FILE\")\n",
    "\n",
    "# Phrases to ignore\n",
    "ignore_phrases = [\n",
    "    \"audio omitted\", \"image omitted\", \"video note omitted\",\n",
    "    \"<This message was edited>\", \"sticker omitted\",\n",
    "    \"video omitted\", \"GIF omitted\",\n",
    "    \"You deleted this message\", \"document omitted\"\n",
    "]\n",
    "\n",
    "# Source name to be used\n",
    "source_name = os.getenv(\"CLON_NAME\")\n",
    "\n",
    "# Define conversation duration in hours\n",
    "conversation_duration = int(os.getenv(\"CONVERSATION_DURATION\"))\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    \"\"\"\n",
    "    Parse the timestamp from the chat log.\n",
    "    \"\"\"\n",
    "    cleaned_timestamp = timestamp.strip().replace('\\u200e', '').replace('\\u202f', '')\n",
    "    cleaned_timestamp = cleaned_timestamp.replace('PM', ' PM').replace('AM', ' AM')  # Add space before AM/PM\n",
    "    return datetime.strptime(cleaned_timestamp, \"[%d/%m/%Y, %I:%M:%S %p]\")\n",
    "\n",
    "def remove_quotes(text):\n",
    "    \"\"\"\n",
    "    Remove quotes from the beginning and end of a string.\n",
    "    \"\"\"\n",
    "    return text.strip('\"')\n",
    "\n",
    "def process_chat(cleaned_file_path, ignore_phrases, source_name, conversation_duration):\n",
    "    \"\"\"\n",
    "    Process the chat log and create one-line conversations for CSV training.\n",
    "    \"\"\"\n",
    "    inputs_outputs = []\n",
    "    current_input = \"\"\n",
    "    current_output = \"\"\n",
    "    last_timestamp = None\n",
    "\n",
    "    with open(cleaned_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if \"]\" in line:\n",
    "                timestamp, rest = line.split(\"]\", 1)\n",
    "                timestamp = timestamp + \"]\"\n",
    "                name, message = rest.split(\":\", 1)\n",
    "                name = name.strip()\n",
    "                message = message.strip()\n",
    "\n",
    "                # Check if the message contains any ignore phrases\n",
    "                if any(phrase in message for phrase in ignore_phrases):\n",
    "                    continue  # Skip this message\n",
    "\n",
    "                current_time = parse_datetime(timestamp)\n",
    "\n",
    "                # Check if more than conversation_duration hours have passed\n",
    "                if last_timestamp and (current_time - last_timestamp > timedelta(hours=conversation_duration)):\n",
    "                    # Save the previous input/output if they exist\n",
    "                    if current_input and current_output:  # Only save if both are filled\n",
    "                        inputs_outputs.append((current_input.strip(), current_output.strip()))\n",
    "                    current_input = \"\"\n",
    "                    current_output = \"\"\n",
    "\n",
    "                last_timestamp = current_time\n",
    "\n",
    "                # Handle messages from other users and source_name\n",
    "                if name != source_name:\n",
    "                    # If source_name has been speaking, save the conversation before adding the new message\n",
    "                    if current_output:\n",
    "                        if current_input and current_output:  # Only save if both input and output exist\n",
    "                            inputs_outputs.append((current_input.strip(), current_output.strip()))\n",
    "                        current_input = \"\"\n",
    "                        current_output = \"\"\n",
    "                    \n",
    "                    # Accumulate input\n",
    "                    current_input += message + \". \"\n",
    "                elif name == source_name:\n",
    "                    # Accumulate source_name's messages\n",
    "                    current_output += message + \". \"\n",
    "\n",
    "        # Save any remaining input/output if not empty\n",
    "        if current_input and current_output:  # Only save if both are filled\n",
    "            inputs_outputs.append((current_input.strip(), current_output.strip()))\n",
    "\n",
    "    return inputs_outputs\n",
    "\n",
    "def write_to_csv(conversation_file_path, inputs_outputs):\n",
    "    \"\"\"\n",
    "    Write the processed inputs and outputs to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(conversation_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Input', 'Output'])\n",
    "\n",
    "        for input_text, output_text in inputs_outputs:\n",
    "            if input_text and output_text:  # Ensure both input and output have content\n",
    "                writer.writerow([remove_quotes(input_text), remove_quotes(output_text)])\n",
    "\n",
    "# Process the chat log and save the training data to a CSV file\n",
    "inputs_outputs = process_chat(cleaned_file_path, ignore_phrases, source_name, conversation_duration)\n",
    "write_to_csv(conversation_file_path, inputs_outputs)\n",
    "print(f\"Training data saved to {conversation_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the dataset\n",
    "#\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# File paths\n",
    "conversation_file_path = os.getenv(\"CONVERSATION_CHAT_FILE\")\n",
    "\n",
    "# Load the dataset from the CSV file using pandas\n",
    "df = pd.read_csv(conversation_file_path)\n",
    "\n",
    "# Ensure the dataset has the correct columns\n",
    "df.columns = ['Input', 'Output']\n",
    "\n",
    "# Replace NaN or inf values with an empty string and remove rows with empty strings\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Input'] != '']\n",
    "df = df[df['Output'] != '']\n",
    "\n",
    "# Optionally limit to 1000 records for testing\n",
    "#df = df[:1000]\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Total records after processing: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Configure training\n",
    "#\n",
    "\n",
    "#import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_polynomial_decay_schedule_with_warmup\n",
    ")\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()   \n",
    "\n",
    "# Constants\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "MODEL_NAME = \"vgaraujov/t5-base-spanish\"\n",
    "OUTPUT_MODEL_DIR = os.getenv(\"OUTPUT_MODEL_DIR\")\n",
    "MAX_LENGTH = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 6\n",
    "WEIGHT_DECAY = 0.001\n",
    "LOGGING_STEPS = 100\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "SAVE_STEPS = 1000\n",
    "FP16 = False\n",
    "REPORT_TO = \"tensorboard\"\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "\n",
    "# Set pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the input and output text.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['Input'], \n",
    "        text_target=examples['Output'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "def log_filter_results(dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Log the number of filtered elements in the dataset.\n",
    "    \"\"\"\n",
    "    original_count = len(dataset)\n",
    "    filtered_dataset = dataset.filter(lambda example: \n",
    "        not np.any(np.isnan(example['input_ids'])) and \n",
    "        not np.any(np.isnan(example['labels'])) and \n",
    "        not np.any(np.isinf(example['input_ids'])) and \n",
    "        not np.any(np.isinf(example['labels']))\n",
    "    )\n",
    "    filtered_count = len(filtered_dataset)\n",
    "    print(f\"{dataset_name} - Filtered out {original_count - filtered_count} elements (from {original_count} total).\")\n",
    "    return filtered_dataset\n",
    "\n",
    "# Tokenize training and evaluation datasets\n",
    "train_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "eval_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Apply filtering to both datasets and log the results\n",
    "train_dataset = log_filter_results(train_dataset, \"Training Dataset\")\n",
    "eval_dataset = log_filter_results(eval_dataset, \"Evaluation Dataset\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Use DataCollator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Training arguments setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=os.path.join(OUTPUT_MODEL_DIR, 'logs'),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    fp16=FP16,\n",
    "    report_to=REPORT_TO,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    lr_scheduler_type=\"polynomial\",  # Use polynomial scheduler\n",
    ")\n",
    "\n",
    "print(\"Train preparation completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Setup and train the model\n",
    "#\n",
    "\n",
    "#import os\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()  \n",
    "\n",
    "def setup_optimizer_and_scheduler(model, training_args, train_dataset):\n",
    "    \"\"\"\n",
    "    Setup the optimizer and polynomial decay scheduler for training.\n",
    "    \"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "\n",
    "    total_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "\n",
    "    # Use polynomial decay instead of linear decay\n",
    "    scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        num_training_steps=total_training_steps,\n",
    "        lr_end=1e-7,\n",
    "        power=2.0 \n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def setup_trainer(model, training_args, train_dataset, eval_dataset, data_collator, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    Setup the Trainer with the optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "\n",
    "def train_and_evaluate(trainer):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model.\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        trainer.model.to('cuda')\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        print(\"Training completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        print(\"Starting evaluation...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Evaluation results: {eval_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "def save_model_and_tokenizer(trainer, tokenizer, OUTPUT_MODEL_DIR):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        trainer.save_model(OUTPUT_MODEL_DIR)\n",
    "        print(\"Model saved.\")\n",
    "        tokenizer.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "        print(\"Tokenizer saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during saving model/tokenizer: {e}\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(model, training_args, train_dataset)\n",
    "# Setup trainer\n",
    "trainer = setup_trainer(model, training_args, train_dataset, eval_dataset, data_collator, optimizer, scheduler)\n",
    "# Train and evaluate\n",
    "train_and_evaluate(trainer)\n",
    "# Save model and tokenizer\n",
    "save_model_and_tokenizer(trainer, tokenizer, OUTPUT_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test text generation\n",
    "#\n",
    "\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "# Path to the directory where the trained model is stored\n",
    "model_dir = os.getenv(\"OUTPUT_MODEL_DIR\")  # Adjust as needed\n",
    "\n",
    "# Load the tokenizer and model from the directory\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# Set padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ensure the model is moved to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=200, num_return_sequences=1, temperature=0.9, top_k=50, top_p=0.95):\n",
    "    try:\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            raise ValueError(\"The input prompt is not valid.\")\n",
    "        \n",
    "        # Tokenize the input prompt with appropriate padding\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt', padding='longest').to('cuda')\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=num_return_sequences, \n",
    "            temperature=temperature, \n",
    "            top_k=top_k, \n",
    "            top_p=top_p,\n",
    "            do_sample=True  # Enable sampling for more diverse outputs\n",
    "        )\n",
    "        \n",
    "        # Decode the generated text, ensuring special tokens like <pad> are skipped\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for output in outputs]\n",
    "        \n",
    "        return generated_texts\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "        print(f\"Error during text generation: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the text generation with a prompt\n",
    "prompt = \"Test Prompt\"\n",
    "generated_texts = generate_text(prompt, num_return_sequences=1, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"{os.getenv(\"CLON_NAME\")}: {i+1}: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
